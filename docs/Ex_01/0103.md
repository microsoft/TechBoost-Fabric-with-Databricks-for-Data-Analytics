---
title: '3. Data ingestion'
layout: default
nav_order: 3
parent: 'Exercise 1: Data Engineering experience, including Data ingestion from a spectrum of analytical data sources into OneLake'
---

# Task 1.3: Data ingestion

There are multiple ways to ingest data in Lakehouse.

## Method 1-Using Data Pipelines/Data Flow ‘No Code-Low Code experience’

1. In Power BI tab, in the ContosoSales@lab.LabInstance.Id workspace, select the **left bottom icon** and select **Data Engineering**.

	![DE.](../media/instructions240153/task-1.3.1.png)

2. In Data Engineering, select **Data pipeline**.

	![Data pipeline 2.png](../media/instructions249094/DataPipeline2.png)

3. In the pipeline **Name** box, enter **Azure SQL DB Pipeline** and select **Create**.

4. Select **Copy data**.

5. In the pop-up, scroll down through the resources, select **Azure SQL Database** and then select **Next**.

6. Select **Create new connection**, if not selected.


7. In the **Server** field paste the value and , select **@lab.Variable(sqlEndpoint)** and in the **Database** field , select **SalesDb**.


8. Scroll down and select **Basic** for Authentication kind, select **labsqladmin** as the Username, **Smoothie@2023** as the Password, keep **Use encrypted connection** check box **on/checked** and finally select **Next**.

	{: .note }
 	>Close any pop-up that you see and wait for the connection to be created.

9. Select **Tables**, select the checkbox for the **First Table** below the **Select all** option, and then select **Next**.

10. Scroll down and select **Lakehouse** and then select **Next**.

11. Select **Existing Lakehouse**, select the **dropdown**, select **lakehouseBronze** and then select **Next**.

12. Select **Load to new table**, select the checkbox beside **Source** and then select **Next**.

13. Select **Save + Run**.

14. Select the **Notifications Icon** at the top right of the screen, select **Notifications**.

15. Verify the **Running status** of the pipeline.

	{: .note }
 	>Please wait for the pipeline to execute. If the notification continues to say running after 10 minutes, check the monitoring hub for a "succeeded" status.
	>![0faouzwm.png](../media/instructions249094/0faouzwm.png)

16. Once the status shows **Run Succeeded**, your data has been transferred from Azure SQL Database to Lakehouse.

17. Similarly you can get data into the lakehouses using pipelines from various other sources like Snowflake, Dataverse, etc. 

## Method 2-Using the OneLake Shortcuts option from external data sources

This is something exciting! You will see how easy it is to create **shortcuts** without actually moving the data. That is the power of OneLake! In this exercise you will easily ingest the curated marketing data as well as product reviews data from ADLS Gen2. Let's see how!

1. In **Power BI** tab, select **Workspaces** and select the ContosoSales@lab.LabInstance.Id workspace.


2. In the ContosoSales@lab.LabInstance.Id workspace, select the **lakehouseBronze** Lakehouse.


	{: .note }
 	>There will be 3 options for lakehouseBronze, namely Lakehouse, Dataset (default) and SQL endpoint. Make sure to select the Lakehouse option.
 	>![ub6haecs.png](../media/instructions249094/ub6haecs.png)


3. Right-click **Files** in the left explorer.

4. Select **New shortcut**.

5. In the pop-up window, under **External sources**, select the **Azure Data Lake Storage Gen2** source.

	{: .note }
 	>Wait for the screen to load.

6. We need to enter the connection details for the ADLS Gen2 shortcut. For this, we need to get the details from the Storage Account resource.

7. In the URL field above, paste the endpoint , select  **https://storage@lab.LabInstance.Id.dfs.core.windows.net/** **URL** field.

8. In the **Authentication kind** dropdown, select **Account Key**.

9. Paste the account key , select **@lab.Variable(storageAccountKey)**.

	{: .note }
 	> Wait for the paste action to complete. You will know it is done when the cursor displays as blinking.

10. Select **Next**.

11. Under **Shortcut Name**, enter **data**.

12. Under **Sub Path**, select **/adlsfabricshortcut**.

13. Select **Create**.


Litware Inc. has curated marketing data and sales data processed by Azure Databricks and stored in the gold layer in ADLS Gen2. You can easily create a shortcut in Microsoft Fabric without moving this data. **We will now create another shortcut for Litware Inc. data.**

14. Right-click **Files** in the left explorer.

15. Select **New shortcut**.

16. In the pop-up window, under **External sources**, select the **Azure Data Lake Storage Gen2** source.

	{: .note }
 	>Wait for the screen to load.

17. In the URL field above, paste the endpoint under the **URL** field , select **https://storage@lab.LabInstance.Id.dfs.core.windows.net/**

	{: .note }
 	>The details entered earlier will be auto fetched. If not, the previous shortcut steps.

18. In the **Authentication kind** dropdown, select **Account Key**.

	{: .warning }
 	> If the account key has already been populated, you do not need to enter it again.

19. Paste the account key , select **@lab.Variable(storageAccountKey)**.

	{: .note }
 	>Wait for the storage AccountKey to be field automatically.

20. Select **Next**.

21. Under **Shortcut Name**, type , select **sales-transaction-litware**.

22. Under **Sub Path**, select **/bronzeshortcutdata**.

23. Select **Create**.
