### Task 3.1: Build ML models, experiments, and Log ML model in the built-in model registry using MLflow and batch scoring.

#### Databricks

The architecture diagram shown here illustrates the end-to-end MLOps pipeline using the Azure Databricks managed MLflow. 

After multiple iterations with various hyperparameters, the best performing model is registered in the Databricks MLflow model registry. Then it is set up as the model serving in the Azure Databricks Workspace for low latency requests.


1. Navigate back to the **Databricks workspace tab** we started for the previous exercise.

2. In the left navigation pane, select **Workspace** and click **Workspace** again. Click on the **03_ML_Solutions_in_a_Box.ipynb** notebook.

	Now that we've processed our customer data, let us use Machine learning model to predict customer churn.

	Ultimately, we would like to understand our customers' sentiment so we can create targeted campaigns to improve our sales.

3. Navigate to **cmd 10**.

	With the data prepared, we can begin exploring the patterns it contains. 

	As illustrated in this chart, we can see a high churn rate is seen if the customer tenure is low, and they have a lower spend amount.

4. Navigate to **cmd 20**.

5. Navigate to **cmd 21**. 

	By registering this model in Model Registry, we can easily reference the model from anywhere within Databricks. 

6. Review the **cmd 29** cell.

	This comparison of multiple runs using a parallel coordinates plot, shows the impact of different parameter values on a metric.

	The best ML model for Customer Churn is selected and registered with Databricks model registry.

7. Navigate to **cmd 40**.

	It is then used to predict the probability of Customer Churn using the deployed model and this model endpoint is ready for production.

   ![Close the browser.](instructions240153/task-3.1.8.png)

8. Navigate to **cmd 41**. 

	Once we have the predicted data, it is stored back in delta tables in the gold layer back in OneLake.

### Data ingestion: Notebook Code-first experience

As Data Engineer,you will explorer another option for ingesting the data. This time Eva prefers using the code-first experience. 
We wi
1. While you are in the **Power BI workspace**, click on the bottom left corner and select **Data Engineering**.

	![DE.](instructions240153/task-1.3.1.png)

2. Click on **Import notebook**.

	![Datawarehouse.](instructions240153/task-1.3-notebook6.png)
	
3. In the **Import Status**, click on the **Upload** button.
	
4. To browse the notebooks from your virtual machine, open file explorer. Click on the whitespace in the **files search bar**, type the path by clicking on **+++C:\Ignite Assets\IgniteDreamLab2023\artifacts\fabricnotebooks+++**, **select all of the notebooks** and click on the **Open** button in your file explorer.

5. Click on the **notification** icon to check the upload status. 

6. Once the upload is complete, the notification will display **Imported successfully**. Click on **Go to Workspace**.

7. Go to the **ContosoSales@lab.LabInstance.Id** workspace and click on the **01 Marketing Data to Lakehouse (Bronze) - Code-First Experience** notebook.

8. In the left pane, click on the **Missing Lakehouse** button and select **Remove all Lakehouses**.

	>**Note:** If you do not see Missing lakehouse, you may see **lakehouse{Name}**, click on it to get the **Remove all Lakehouses** option.

9. Click on **Continue** in the pop-up window.

10. In the left navigation pane, click on the **Add** button.

11. In the pop-up, select the **Existing Lakehouse** radio button and then click on the **Add** button.

12. Click on the **lakehouseBronze** checkbox and then click on **Add**.

	![Datawarehouse.](instructions240153/task-1.3-notebook-15.png)

13. Close the **Information box** for a better view of the notebook content.

14. Go to the cell with name **Shortcut Folder Path**, replace **#WORKSPACE_NAME#** by clicking on +++**ContosoSales@lab.LabInstance.Id**+++.

	>**Note**: Make sure you delete the **"#"** too in above step.
		![Close the browser.](instructions240153/task-1.3-notebook-18.png)

15. Click on **Run all**

#### SQL Query

1. Click on the **New SQL Query** button.

	![Select Notebook](instructions240153/task-2.3-sql4.png)

2. Copy the **SQL query** by clicking on 
	```
	Select fc.ProductCategory,Sum(fc.Revenue) Revenue from [lakehouseSilver].[dbo].[dimension_product] dp JOIN [lakehouseSilver].[dbo].[fact_campaigndata] fc on dp.Category=fc.[ProductCategory] Group by fc.ProductCategory having sum(fc.[Revenue])< (Select Top 1 Sum(fc.Revenue) Revenue from [lakehouseSilver].[dbo].[dimension_product] dp JOIN [lakehouseSilver].[dbo].[fact_campaigndata] fc on dp.Category=fc.[ProductCategory] Group by fc.ProductCategory ORDER by Revenue DESC) ORDER by Revenue DESC 
	```

3. Click on **Run** icon to view the query result.
	!IMAGE[task-2.3-sql44.png](instructions240153/task-2.3-sql44.png)
