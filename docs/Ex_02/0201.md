---
title: '1. Set up an Azure Databricks environment'
layout: default
nav_order: 1
parent: 'Exercise 2: Explore an analytics pipeline using open Delta format and Azure Databricks Delta Live Tables'
---

# Task 2.1: Set up an Azure Databricks environment

**Integrate OneLake with Databricks:**

- **Use OneLake with existing data lakes using Shortcuts**
- **Use and land data directly in OneLake**

In this exercise, you will use second option **Use and land data directly in OneLake**.

Contoso already had some of their compute workload on **Azure Databricks**. You donâ€™t need to migrate any of that workload to work with Fabric. You can simply use the OneLake endpoint to mount the storage and work with the same data directly from the Lakehouse for their analytical and ML operations.

**Currently there are two ways to authenticate OneLake.**
	
- **Credential passthrough**
- **Service Principal approach** (In this lab, you will use Service Principal approach)

1. Open new tab in your browser and sign in to the **Azure Databricks Workspace**, , select **https://@lab.Variable(workspaceurl)** and press **ENTER**.

1. Select **Sign in with Azure AD**.
	
 	![task-2.1.new7.png](../media/instructions240153/task-2.1.new7.png)

    {: .note }
    > Skip or Close any popups that appear.

1. In the left navigation pane, select **Workspace**, select **Workspace** in the Workspace navigation menu and then select the **01_Setup-OneLake_Integration_with_Databrick** notebook.

    {: .note }
    > Skip or Close any popups you see.

1. In the cell named **OneLake Path** or **cmd 2**, replace "#WORKSPACE_NAME#" with the current Fabric workspace name , select **ContosoSales@lab.LabInstance.Id**.

	![Select Workflows](../media/instructions240153/task-2.1.7.png)

1. Select **Run all**. A new window will pop up.

1. Select **Attach and run** to start executing the notebook.

1. Once the setup notebook runs successfully, mounting to the OneLake is complete.

    {: .note }
    > It will take a few minutes for the execution to complete. You can check the last cell of notebook for the last execution time to verify.
